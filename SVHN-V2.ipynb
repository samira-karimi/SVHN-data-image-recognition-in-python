{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "import h5py # binary data format - to store huge amounts\n",
    "import scipy.io as sio # For loading matlab files\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "## from cnn_utils import *\n",
    "# plots images right below the jupyter cell:\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set X and y shapes are  (32, 32, 3, 73257) (73257, 1)\n",
      "Test set X and y shapes are  (32, 32, 3, 26032) (26032, 1)\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "TrainSet = sio.loadmat('train_32x32.mat')\n",
    "train_x_orig, train_y_orig = TrainSet[\"X\"],TrainSet[\"y\"]\n",
    "TestSet = sio.loadmat('test_32x32.mat')\n",
    "test_x_orig, test_y_orig = TestSet[\"X\"],TestSet[\"y\"]\n",
    "print(\"Train set X and y shapes are \", train_x_orig.shape , train_y_orig.shape)\n",
    "print(\"Test set X and y shapes are \", test_x_orig.shape , test_y_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set X and y shapes are  (73257, 32, 32, 3) (73257, 1)\n",
      "Test set X and y shapes are  (26032, 32, 32, 3) (26032, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_x_orig.transpose((-1, 0 , 1, 2))/255\n",
    "test_x = test_x_orig.transpose((-1,0, 1, 2))/255\n",
    "m_train = train_y_orig.shape[0]\n",
    "m_test = test_y_orig.shape[0]\n",
    "print(\"Train set X and y shapes are \", train_x.shape , train_y_orig.shape)\n",
    "print(\"Test set X and y shapes are \", test_x.shape , test_y_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x280f4435438>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.unique(train_y_orig))\n",
    "result = np.where(train_y_orig==10)\n",
    "result # 52,...\n",
    "plt.imshow(train_x[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del train_y\n",
    "# del test_y\n",
    "train_y_mod = train_y_orig\n",
    "test_y_mod = test_y_orig\n",
    "train_y_mod[train_y_mod==10] = 0\n",
    "test_y_mod[test_y_mod==10] = 0\n",
    "train_y_mod[52,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, n):\n",
    "    d1 = Y.shape[0]\n",
    "    res = np.zeros((d1, n))\n",
    "    row = np.arange(d1)\n",
    "    col = Y.T\n",
    "    res[row, col] =1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73257, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = convert_to_one_hot(train_y_mod, 10)\n",
    "test_y = convert_to_one_hot(test_y_mod, 10)\n",
    "print(train_y_mod[10])\n",
    "print(train_y[10,:])\n",
    "train_y.shape\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Needed for re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(X_H, X_W, X_C, n_y):\n",
    "    \"\"\"\n",
    "    n_y: # classes of labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder('float', shape=(None, X_H, X_W, X_C))\n",
    "    Y = tf.placeholder('float', shape=(None, n_y))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_W_parameters(shapes_W): # W_shapes: dictionaty\n",
    "    \n",
    "        L = len(shapes_W)\n",
    "        res = {}\n",
    "        for l in range(L):\n",
    "            res.update({'W' + str(l+1): tf.get_variable('W' + str(l+1), W_shapes['W' + str(l+1)],\\\n",
    "                                                      initializer = tf.contrib.layers.xavier_initializer(seed = 0))})\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_shapes = {\"W1\":(4, 4, 3, 8), \"W2\":(2, 2, 8, 16)}\n",
    "S = {\"S1\":[1, 1, 1, 1], \"S2\":[1, 1, 1, 1]}\n",
    "SP = {\"S1\":[1,8,8,1], \"S2\":[1,4,4,1]}\n",
    "KP = {\"K1\":[1,8,8,1], \"K2\":[1,4,4,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W_init = initialize_W_parameters(W_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': <tf.Variable 'W1:0' shape=(4, 4, 3, 8) dtype=float32_ref>,\n",
       " 'W2': <tf.Variable 'W2:0' shape=(2, 2, 8, 16) dtype=float32_ref>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"forward_propagation\", \"compute_cost\" and \"model\" functions are from deeplearning.ai coursera course numner 4, week 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Note that for simplicity and grading purposes, we'll hard-code some values\n",
    "    such as the stride and kernel (filter) sizes. \n",
    "    Normally, functions should take these values as function parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides = [1,1,1,1], padding='SAME')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    # FLATTEN\n",
    "    F = tf.contrib.layers.flatten(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 10 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(F, 10, activation_fn=None)\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is from [link](https://github.com/AlbertHG/Coursera-Deep-Learning-deeplearning.ai/blob/master/04-Convolutional%20Neural%20Networks/week1/cnn_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(res_fwd_prop, Y):\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = res_fwd_prop, labels=Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, W_shapes, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 10)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 10)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_W_parameters(W_shapes)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \"\"\"\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost.\n",
    "                # The feedict should contain a minibatch for (X,Y).\n",
    "                \"\"\"\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer,cost], feed_dict = {X: minibatch_X, Y: minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Samira\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000028167E58710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000028167E58710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000028167E58710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000028167E58710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028167E58D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028167E58D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028167E58D30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028167E58D30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-16-70ba5bb8de3b>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cost after epoch 0: 2.239517\n",
      "Cost after epoch 5: 2.074587\n",
      "Cost after epoch 10: 2.068611\n",
      "Cost after epoch 15: 2.068339\n",
      "Cost after epoch 20: 2.061843\n",
      "Cost after epoch 25: 2.058672\n",
      "Cost after epoch 30: 2.063659\n",
      "Cost after epoch 35: 2.061268\n",
      "Cost after epoch 40: 2.062446\n",
      "Cost after epoch 45: 2.061866\n",
      "Cost after epoch 50: 2.058556\n",
      "Cost after epoch 55: 2.060405\n",
      "Cost after epoch 60: 2.060231\n",
      "Cost after epoch 65: 2.062952\n",
      "Cost after epoch 70: 2.060895\n",
      "Cost after epoch 75: 2.059524\n",
      "Cost after epoch 80: 2.061105\n",
      "Cost after epoch 85: 2.061420\n",
      "Cost after epoch 90: 2.062036\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(train_x, train_y, test_x, test_y, W_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
